{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learner/Facilitator Guide - Sequential Data Modeling with Tensorflow Keras  - 2 Oct 2021.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xr7i2L08IIp5",
        "jEpIw2V0lmGF",
        "TO3WCmo2nB-Q",
        "3H1vHLHUmUId",
        "vNCS7L2amtYn",
        "av71E0Qcnwon",
        "D6mpaNGixeDu",
        "YRck3uIwyeLo",
        "6naCWJHxygNk",
        "u4eKE0nEyxfk",
        "kMaVg1zTKCIG",
        "1sH7Cj5WKqoI",
        "G_aEYm4PVptX",
        "AliO7-3ZVY0R",
        "6tZ-97-pcFIW",
        "dm-q7PEUcqbM",
        "RTcnxwuMdWST"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaZZDVcrO4gi"
      },
      "source": [
        "# Learner/Facilitator Guide - Sequential Data Modeling with Tensorflow Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLlwee1LY0CA"
      },
      "source": [
        "# Topic 1 Time Series Forecasting using RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46Bs8B1gcbpm"
      },
      "source": [
        "## RNN/LSTM/GRU and Input Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgE8sw3gcUM0"
      },
      "source": [
        "import tensorflow as tf \n",
        "\n",
        "batch = 5 \n",
        "feature = 10 \n",
        "timesteps = 3 \n",
        "\n",
        "inputs = np.random.randn(batch, timesteps, feature)\n",
        "\n",
        "hidden_size = 20 \n",
        "rnn = tf.keras.layers.SimpleRNN(hidden_size)\n",
        "\n",
        "h_out = rnn(inputs)\n",
        "print('Output size (batch, hidden_size) = ', h_out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEjBqqgccWbF"
      },
      "source": [
        "import tensorflow as tf \n",
        "\n",
        "batch = 5 \n",
        "feature = 10\n",
        "timesteps = 3 \n",
        "\n",
        "inputs = np.random.randn(batch, timesteps, feature)\n",
        "\n",
        "hidden_size = 20 \n",
        "lstm = tf.keras.layers.LSTM(hidden_size)\n",
        "\n",
        "h_out = lstm(inputs)\n",
        "print('Output size (batch, hidden_size) = ', h_out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbEPZQjacYjq"
      },
      "source": [
        "import tensorflow as tf \n",
        "\n",
        "batch = 5 \n",
        "feature = 10 \n",
        "timesteps = 3  \n",
        "\n",
        "inputs = np.random.randn(batch, timesteps, feature)\n",
        "\n",
        "hidden_size = 20 \n",
        "gru = tf.keras.layers.GRU(hidden_size)\n",
        "\n",
        "h_out = gru(inputs)\n",
        "print('Output size (batch, hidden_size) = ', h_out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4RGqLxeclQM"
      },
      "source": [
        "## Time Series Forcasting with RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aypqhdw2cozk"
      },
      "source": [
        "### Time Series Forecasting (Airplane Passengers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B9tylB6cyj4"
      },
      "source": [
        "#### Step 1: Preprocess Data (Air Passengers Dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58RGWJpYckZ5"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlH-VpbCc1Ar"
      },
      "source": [
        "dataset = pd.read_csv('https://raw.githubusercontent.com/tertiarycourses/datasets/master/airline-passengers.csv')\n",
        "dataset.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMI_4fuGc3FK"
      },
      "source": [
        "dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBfSNcMMc5LQ"
      },
      "source": [
        "dataset = dataset.iloc[:,1:2].values\n",
        "\n",
        "plt.plot(dataset)\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Passengers')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5IA63Suc8az"
      },
      "source": [
        "def sliding_window(data, seq_length):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(data)-seq_length-1):\n",
        "        _x = data[i:(i+seq_length)]\n",
        "        _y = data[i+seq_length]\n",
        "        x.append(_x)\n",
        "        y.append(_y)\n",
        "\n",
        "    return np.array(x),np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGzYhiK7c-m5"
      },
      "source": [
        "sc = MinMaxScaler()\n",
        "dataset = sc.fit_transform(dataset)\n",
        "\n",
        "timesteps = 4\n",
        "X,y = sliding_window(dataset, timesteps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UPO_FmSdArI"
      },
      "source": [
        "train_size = int(len(y) * 0.67)\n",
        "test_size = int(len(y)) - train_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMGX_A-rdC1H"
      },
      "source": [
        "X_train = X[0:train_size]\n",
        "y_train = y[0:train_size]\n",
        "\n",
        "X_test = X[train_size:len(X)]\n",
        "y_test = y[train_size:len(y)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CSHXiVc8bsK"
      },
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFHs1G5pdE3G"
      },
      "source": [
        "input_size = 1\n",
        "hidden_size = 5\n",
        "\n",
        "# inputs: A 3D tensor with shape [batch, timesteps, feature].\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], timesteps, input_size))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], timesteps, 1, input_size))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mhI7OaEdKj_"
      },
      "source": [
        "#### Step 2: Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNSGsV9edMvy"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(hidden_size, activation='tanh',input_shape=(timesteps,input_size)))\n",
        "model.add(Dense(1,activation='linear'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UmV0JDBdOtX"
      },
      "source": [
        "#### Step 3: Define Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd79Dt-KdQ9f"
      },
      "source": [
        "model.compile(loss='mse', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzQXBQhfdU9w"
      },
      "source": [
        "#### Step 4: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K69uq5dDdVif"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvfifV59dY4t"
      },
      "source": [
        "#### Step 5: Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxPSTx_TdcSj"
      },
      "source": [
        "import pandas as pd \n",
        "\n",
        "loss_df = pd.DataFrame(history.history)\n",
        "loss_df['loss'].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMN2YNFGdewZ"
      },
      "source": [
        "yhat = model(X)\n",
        "\n",
        "yhat = sc.inverse_transform(yhat)\n",
        "y_ = sc.inverse_transform(y)\n",
        "\n",
        "plt.axvline(x=train_size, c='g', linestyle='--')\n",
        "\n",
        "plt.plot(y_,'b',label='actual')\n",
        "plt.plot(yhat,'r',label='prediction')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Airline Passengers')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eews32uz-U1s"
      },
      "source": [
        "### Activity: Sales Price Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5srntLjB3Fj"
      },
      "source": [
        "#### Step 1: Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZGSfd0B-X7E"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/tertiarycourses/datasets/master/shampoo.csv')\n",
        "dataset.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEsaY3hy-h_q"
      },
      "source": [
        "dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMsQZDgyAVTG"
      },
      "source": [
        "dataset = dataset.iloc[:,1:2].values\n",
        "\n",
        "plt.plot(dataset)\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Sales')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0gaF4wHApAy"
      },
      "source": [
        "def sliding_window(data, seq_length):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(data)-seq_length-1):\n",
        "        _x = data[i:(i+seq_length)]\n",
        "        _y = data[i+seq_length]\n",
        "        x.append(_x)\n",
        "        y.append(_y)\n",
        "\n",
        "    return np.array(x),np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjC9Jup5AwnX"
      },
      "source": [
        "sc = MinMaxScaler()\n",
        "dataset = sc.fit_transform(dataset)\n",
        "\n",
        "timesteps = 4\n",
        "X,y = sliding_window(dataset, timesteps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSJNCqHfA1yR"
      },
      "source": [
        "train_size = int(len(y) * 0.67)\n",
        "test_size = int(len(y)) - train_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxB_Usq1A7x5"
      },
      "source": [
        "X_train = X[0:train_size]\n",
        "y_train = y[0:train_size]\n",
        "\n",
        "X_test = X[train_size:len(X)]\n",
        "y_test = y[train_size:len(y)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQbRipLYA_5i"
      },
      "source": [
        "train_size = int(len(y) * 0.67)\n",
        "test_size = int(len(y)) - train_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzIhF6jdBCo9"
      },
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ahRDn4tBDhj"
      },
      "source": [
        "input_size = 1\n",
        "hidden_size = 5\n",
        "\n",
        "# inputs: A 3D tensor with shape [batch, timesteps, feature].\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], timesteps, input_size))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], timesteps, 1, input_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aX27HXcB7mB"
      },
      "source": [
        "#### Step 2: Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlySt09oBNvI"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(hidden_size, activation='tanh',input_shape=(timesteps,input_size)))\n",
        "model.add(Dense(1,activation='linear'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2rHC-sDB-OA"
      },
      "source": [
        "#### Step 3: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-cQ5DpJBQ5_"
      },
      "source": [
        "model.compile(loss='mse', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYsxuDAnBXNr"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BX0JD9KCDXy"
      },
      "source": [
        "#### Step 4: Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuL5kCpyBjsa"
      },
      "source": [
        "import pandas as pd \n",
        "\n",
        "loss_df = pd.DataFrame(history.history)\n",
        "loss_df['loss'].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEiaYPPpBn0I"
      },
      "source": [
        "yhat = model(X)\n",
        "\n",
        "yhat = sc.inverse_transform(yhat)\n",
        "y_ = sc.inverse_transform(y)\n",
        "\n",
        "plt.axvline(x=train_size, c='g', linestyle='--')\n",
        "\n",
        "plt.plot(y_,'b',label='actual')\n",
        "plt.plot(yhat,'r',label='prediction')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Airline Passengers')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hDc_rCmdj-Y"
      },
      "source": [
        "### Activity: Stock Price Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hosmF9HZdn_C"
      },
      "source": [
        "#### Step 1: Preprocess the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3gGKW5Cdp17"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw183kjMdrr3"
      },
      "source": [
        "dataset = pd.read_csv('https://raw.githubusercontent.com/tertiarycourses/datasets/master/DBS.csv',usecols=['Date','Close'])\n",
        "\n",
        "dataset.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5guCkWcgdtSo"
      },
      "source": [
        "dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2eME0oadvCF"
      },
      "source": [
        "dataset = dataset.iloc[:,1:2].values\n",
        "\n",
        "plt.plot(dataset)\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Sales')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuMHbwKsdw1z"
      },
      "source": [
        "def sliding_window(data, seq_length):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(data)-seq_length-1):\n",
        "        _x = data[i:(i+seq_length)]\n",
        "        _y = data[i+seq_length]\n",
        "        x.append(_x)\n",
        "        y.append(_y)\n",
        "\n",
        "    return np.array(x),np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWNfgDRGdyoj"
      },
      "source": [
        "sc = MinMaxScaler()\n",
        "dataset = sc.fit_transform(dataset)\n",
        "\n",
        "timesteps = 4\n",
        "X,y = sliding_window(dataset, timesteps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPdmTfAxd0Wz"
      },
      "source": [
        "train_size = int(len(y) * 0.67)\n",
        "test_size = int(len(y)) - train_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIcfd7uVd2Ut"
      },
      "source": [
        "X_train = X[0:train_size]\n",
        "y_train = y[0:train_size]\n",
        "\n",
        "X_test = X[train_size:len(X)]\n",
        "y_test = y[train_size:len(y)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XwEt4RFd4Sw"
      },
      "source": [
        "input_size = 1\n",
        "hidden_size = 5\n",
        "\n",
        "# inputs: A 3D tensor with shape [batch, timesteps, feature].\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], timesteps, input_size))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], timesteps, 1, input_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNgK4aKmd6db"
      },
      "source": [
        "#### Step 2: Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJkhtwUVd7O4"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(hidden_size, activation='tanh',input_shape=(timesteps,input_size)))\n",
        "model.add(Dense(1,activation='linear'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-2p8bgCd9HI"
      },
      "source": [
        "#### Step 3: Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HABPQcUpd_EV"
      },
      "source": [
        "model.compile(loss='mse', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7ElAsR0eBAF"
      },
      "source": [
        "#### Step 4: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HHB6hwAeDDD"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKMWiarYeGd3"
      },
      "source": [
        "#### Step 5: Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKYn2qwDeInB"
      },
      "source": [
        "import pandas as pd \n",
        "\n",
        "loss_df = pd.DataFrame(history.history)\n",
        "loss_df['loss'].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9l6fvEIeLDf"
      },
      "source": [
        "yhat = model(X)\n",
        "\n",
        "yhat = sc.inverse_transform(yhat)\n",
        "y_ = sc.inverse_transform(y)\n",
        "\n",
        "plt.axvline(x=train_size, c='g', linestyle='--')\n",
        "\n",
        "plt.plot(y_,'b',label='actual')\n",
        "plt.plot(yhat,'r',label='prediction')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Stock Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2wBoLstdYvd"
      },
      "source": [
        "# Topic 2 Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-EJJ8zX-o3I"
      },
      "source": [
        "## Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wct5567wd9rj"
      },
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams=1,\n",
        "    max_tokens=200,\n",
        "    output_mode=\"int\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W76TxeKgeAOq"
      },
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "text_vectorization.adapt(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1PjKnEheFBj"
      },
      "source": [
        "text_vectorization.get_vocabulary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntI9w5b5eGvD"
      },
      "source": [
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIwnk5uYeIZd"
      },
      "source": [
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(decoded_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKpLlb_nfXKh"
      },
      "source": [
        "## Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCrDDa1BfcOq"
      },
      "source": [
        "##import the required libraries and APIs\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDxl43wVffmf"
      },
      "source": [
        "### Downloading the TensorFlow `imdb_review` dataset\n",
        "\n",
        "> Make sure tensorflow_datasets is installed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTfXiPOwfhGy"
      },
      "source": [
        "##load the imdb reviews dataset\n",
        "data, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpSPGO_1gDmu"
      },
      "source": [
        "### Segregating training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gV0hOBBf5bV"
      },
      "source": [
        "##segregate training and test set\n",
        "train_data, test_data = data['train'], data['test']\n",
        "\n",
        "##create empty list to store sentences and labels\n",
        "train_sentences = []\n",
        "test_sentences = []\n",
        "\n",
        "train_labels = []\n",
        "test_labels = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb167zIwgHKd"
      },
      "source": [
        "##iterate over the train data to extract sentences and labels\n",
        "for sent, label in train_data:\n",
        "    train_sentences.append(str(sent.numpy().decode('utf8')))\n",
        "    train_labels.append(label.numpy())\n",
        "\n",
        "##iterate over the test set to extract sentences and labels\n",
        "for sent, label in test_data:\n",
        "    test_sentences.append(str(sent.numpy().decode('utf8')))\n",
        "    test_labels.append(label.numpy())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iLtdjQdgNoc"
      },
      "source": [
        "print(train_sentences[0])\n",
        "print(train_labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynByrSdfgc9-"
      },
      "source": [
        "##convert lists into numpy array\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6ZU4DH5ggNX"
      },
      "source": [
        "### Data preparation - setting up the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS9ZMld6giqT"
      },
      "source": [
        "##define the parameters for the tokenizing and padding\n",
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "max_length = 120\n",
        "trunc_type='post'\n",
        "oov_tok = \"<OOV>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvKJrDNvgrnh"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "##training sequences and labels\n",
        "train_seqs = tokenizer.texts_to_sequences(train_sentences)\n",
        "train_padded = pad_sequences(train_seqs,maxlen=max_length, truncating=trunc_type)\n",
        "\n",
        "##testing sequences and labels\n",
        "test_seqs = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = pad_sequences(test_seqs,maxlen=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRPNGLWog4Mn"
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "print(train_sentences[1])\n",
        "print(train_padded[1])\n",
        "print(decode_review(train_padded[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAN0mVL6hUZ0"
      },
      "source": [
        "### Define the Neural Network with Embedding layer\n",
        "\n",
        "1. Use the Sequential API.\n",
        "2. Add an embedding input layer of input size equal to vocabulary size.\n",
        "3. Add a flatten layer, and two dense layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COfZXVfehXAP"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU269Ufjhb3M"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wDAMd-uhZJP"
      },
      "source": [
        "##compile the model with loss function, optimizer and metrics\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mv0MXTwhhR7"
      },
      "source": [
        "num_epochs = 10\n",
        "\n",
        "##train the model with training and validation set\n",
        "model.fit(\n",
        "    train_padded, \n",
        "    train_labels, \n",
        "    epochs=num_epochs, \n",
        "    validation_data=(test_padded, test_labels)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGrcIjlzh_yR"
      },
      "source": [
        "### Deriving weights from the embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYePFUw5iB0N"
      },
      "source": [
        "##isolating the first embedding layer\n",
        "embedding = model.layers[0]\n",
        "\n",
        "##extracting learned weights\n",
        "weights = embedding.get_weights()[0]\n",
        "print(weights.shape) # shape: (vocab_size, embedding_dim)\n",
        "print(weights[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbg0XjsTjZB2"
      },
      "source": [
        "##import I/O module in python\n",
        "import io\n",
        "\n",
        "##open the text stream for vectors\n",
        "vectors = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "##open the text stream for metadata\n",
        "meta = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "\n",
        "##write each word and its corresponding embedding\n",
        "for index in range(1, vocab_size):\n",
        "  word = reverse_word_index[index]  # flipping the key-value in word_index\n",
        "  embeddings = weights[index]\n",
        "  meta.write(word + \"\\n\")\n",
        "  vectors.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
        "\n",
        "##close the stream\n",
        "vectors.close()\n",
        "meta.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYyTuvJRjcPH"
      },
      "source": [
        "##download the written files to your local machine\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download('vectors.tsv')\n",
        "  files.download('meta.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oftG5ka_YMyS"
      },
      "source": [
        "## Text Classifcation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w444RHUxPG2B"
      },
      "source": [
        "##import the required libraries and APIs\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTH6bmh5PLyt"
      },
      "source": [
        "### Downloading the News Headlines data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc-r6FBpPQQN"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/wdd-2-node.appspot.com/x1.json \\\n",
        "    -o /tmp/headlines.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bZ1Ni3NPW78"
      },
      "source": [
        "##read the the json file using pandas\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_json(\"./x1.json\")\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iwboda6RP0cd"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsc9arTpP326"
      },
      "source": [
        "##store headlines and labels in respective lists\n",
        "headlines = list(data['headline'])\n",
        "labels = list(data['is_sarcastic'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xgt5wG7QBEL"
      },
      "source": [
        "### Set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTx_XHTWP-rj"
      },
      "source": [
        "##define tokenizing and padding parameters\n",
        "vocab_size = 10000\n",
        "max_length = 120\n",
        "embedding_dim = 16\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "training_size = 20000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5xiEVeVP7yu"
      },
      "source": [
        "### Splitting the training and testing set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh_QC5pTQURl"
      },
      "source": [
        "##sentences\n",
        "training_sentences = headlines[0:training_size]\n",
        "testing_sentences = headlines[training_size:]\n",
        "\n",
        "##labels\n",
        "training_labels = labels[0:training_size]\n",
        "testing_labels = labels[training_size:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE82seybQYz1"
      },
      "source": [
        "### Preprocess sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW_P01ysQaM6"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmSr0SsPQhHm"
      },
      "source": [
        "# convert lists into numpy arrays to make it work with TensorFlow 2.x\n",
        "training_padded = np.array(training_padded)\n",
        "training_labels = np.array(training_labels)\n",
        "testing_padded = np.array(testing_padded)\n",
        "testing_labels = np.array(testing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7V-Ahh1QmT4"
      },
      "source": [
        "### Define the neural network model with the following layers:\n",
        "1. Embedding layer\n",
        "2. Global Average pooling layer(1D)\n",
        "3. Dense layer with 24 nodes\n",
        "4. Output Dense layer with `sigmoid` activation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRCNfl0BQn7l"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "##compile the model\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffSeFloEQsTr"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oks59Ez8QvIE"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Save it into history"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDU_wuBhQyE4"
      },
      "source": [
        "num_epochs = 30\n",
        "history = model.fit(training_padded, \n",
        "                    training_labels, \n",
        "                    epochs=num_epochs, \n",
        "                    validation_data=(testing_padded, testing_labels), \n",
        "                    verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCMYz4tVTrnd"
      },
      "source": [
        "### Visualise the train & validation accuracy and loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YIC9vX5TtLc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "##plot the scores from history\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f80gFqJUTy6k"
      },
      "source": [
        "### Classifying a new sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GFXUgDoT0Wg"
      },
      "source": [
        "sentence = [\"the baby boy fears spiders in the garden might be real\", \"game of thrones season finale showing this sunday night\"]\n",
        "\n",
        "##prepare the sequences of the sentences in question\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "padded_seqs = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "print(model.predict(padded_seqs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js8LTEl2xCNh"
      },
      "source": [
        "### Activity: Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlsLDfmVxIyJ"
      },
      "source": [
        "##import the required libraries and APIs\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgpD57VRxNcH"
      },
      "source": [
        "#### Downloading the TensorFlow `yelp_popularity_review` dataset\n",
        "\n",
        "> Make sure tensorflow_datasets is installed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTvwJpPvxMWW"
      },
      "source": [
        "##load the yelp reviews dataset\n",
        "data, info = tfds.load(\"yelp_polarity_reviews\", with_info=True, as_supervised=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3-Ds6cFxVGN"
      },
      "source": [
        "train_data, test_data = data['train'], data['test']\n",
        "\n",
        "train_sentences = []\n",
        "test_sentences = []\n",
        "\n",
        "train_labels = []\n",
        "test_labels = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE7XH1z5xWzQ"
      },
      "source": [
        "for sent, label in train_data:\n",
        "    train_sentences.append(str(sent.numpy().decode('utf8')))\n",
        "    train_labels.append(label.numpy())\n",
        "\n",
        "for sent, label in test_data:\n",
        "    test_sentences.append(str(sent.numpy().decode('utf8')))\n",
        "    test_labels.append(label.numpy())\n",
        "\n",
        "\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOC79q-JxYZV"
      },
      "source": [
        "##define the parameters for tokenizing and padding\n",
        "vocab_size = ____\n",
        "embedding_dim = __\n",
        "max_length = ____\n",
        "padding_type = 'post'\n",
        "trunc_type='post'\n",
        "oov_tok = \"<OOV>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1694fki1xaUV"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "##training sequences and labels\n",
        "train_seqs = tokenizer.texts_to_sequences(train_sentences)\n",
        "train_padded = pad_sequences(train_seqs,maxlen=max_length, truncating=trunc_type)\n",
        "\n",
        "##testing sequences and labels\n",
        "test_seqs = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = pad_sequences(test_seqs,maxlen=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSUHFmRrxciV"
      },
      "source": [
        "#### Explore the LSTM & CNN model with the following layers:\n",
        "1. Embedding layer\n",
        "2. Try two bidirectional LSTM layers or a Conv1D layer or both.\n",
        "3. Dense layer with 24 nodes\n",
        "4. Output Dense layer with `sigmoid` activation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8MCSGZoxepN"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.____(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.____(tf.keras.layers.LSTM(____, return_sequences=True)),\n",
        "    tf.keras.layers.____(tf.keras.layers.LSTM(__)),\n",
        "    tf.keras.layers.____(32, activation='relu'),\n",
        "    tf.keras.layers.____(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCij5hhFxhgK"
      },
      "source": [
        "num_epochs = 10\n",
        "history = model.fit(\n",
        "    train_padded, \n",
        "    train_labels, \n",
        "    epochs=num_epochs, \n",
        "    validation_data=(test_padded, test_labels)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4HpWbLWxj1R"
      },
      "source": [
        "#### Visualise the accuracy and loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XY2LtRVxos8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_metrics(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])\n",
        "  plt.show()\n",
        "  \n",
        "plot_metrics(history, \"accuracy\")\n",
        "plot_metrics(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSXwyKcYxwAd"
      },
      "source": [
        "#### Classify new reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__A0h4t-xttb"
      },
      "source": [
        "sentence = [\"the restaurant served a delicious pasta\", \"the restaurant didn't have a decent ambience\"]\n",
        "sequences = tokenizer.____(sentence)\n",
        "padded = ____(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "print(model.____(padded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLk8sJtktwz9"
      },
      "source": [
        "### Solution: Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05KUIjzitzeh"
      },
      "source": [
        "##import the required libraries and APIs\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh2YEsYXuQie"
      },
      "source": [
        "##load the yelp reviews dataset\n",
        "data, info = tfds.load(\"yelp_polarity_reviews\", with_info=True, as_supervised=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73e4OLBKuXaw"
      },
      "source": [
        "train_data, test_data = data['train'], data['test']\n",
        "\n",
        "train_sentences = []\n",
        "test_sentences = []\n",
        "\n",
        "train_labels = []\n",
        "test_labels = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14VbTs8cu2wF"
      },
      "source": [
        "for sent, label in train_data:\n",
        "    train_sentences.append(str(sent.numpy().decode('utf8')))\n",
        "    train_labels.append(label.numpy())\n",
        "\n",
        "for sent, label in test_data:\n",
        "    test_sentences.append(str(sent.numpy().decode('utf8')))\n",
        "    test_labels.append(label.numpy())\n",
        "\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DDcT71eu4r2"
      },
      "source": [
        "##define the parameters for tokenizing and padding\n",
        "vocab_size = 10000\n",
        "embedding_dim = 32\n",
        "max_length = 120\n",
        "padding_type = 'post'\n",
        "trunc_type='post'\n",
        "oov_tok = \"<OOV>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ehz_aWw9u7Zz"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "##training sequences and labels\n",
        "train_seqs = tokenizer.texts_to_sequences(train_sentences)\n",
        "train_padded = pad_sequences(train_seqs,maxlen=max_length, truncating=trunc_type)\n",
        "\n",
        "##testing sequences and labels\n",
        "test_seqs = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = pad_sequences(test_seqs,maxlen=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJKQ917gwSJq"
      },
      "source": [
        "#### Explore the LSTM & CNN model with the following layers:\n",
        "1. Embedding layer\n",
        "2. Try two bidirectional LSTM layers or a Conv1D layer or both.\n",
        "3. Dense layer with 24 nodes\n",
        "4. Output Dense layer with `sigmoid` activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5T_AHTHwPS9"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uYmL0_WwVRf"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr97zrV2wbGq"
      },
      "source": [
        "num_epochs = 10\n",
        "history = model.fit(\n",
        "    train_padded, \n",
        "    train_labels, \n",
        "    epochs=num_epochs, \n",
        "    validation_data=(test_padded, test_labels)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX07ZHFPe66h"
      },
      "source": [
        "## Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsU9Sc9qZOL2"
      },
      "source": [
        "##import the required libraries and APIs\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqUCKGZcfIoz"
      },
      "source": [
        "### Step 1: Create a corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6p0Xr1VfKNd"
      },
      "source": [
        "data = \"October arrived, spreading a damp chill over the grounds and into the castle.\\n Madam Pomfrey, the nurse, was kept busy by a sudden spate of colds among the staff and students.\\n Her Pepperup potion worked instantly, though it left the drinker smoking at the ears for several hours afterward. Ginny Weasley, who had been looking pale, was bullied into taking some by Percy.\\n The steam pouring from under her vivid hair gave the impression that her whole head was on fire.\\n Raindrops the size of bullets thundered on the castle windows for days on end; the lake rose, the flower beds turned into muddy streams, and Hagrid's pumpkins swelled to the size of garden sheds.\\n Oliver Wood's enthusiasm for regular training sessions, however, was not dampened, which was why Harry was to be found, late one stormy Saturday afternoon a few days before Halloween, returning to Gryffindor Tower, drenched to the skin and splattered with mud.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnP-rqllfXwZ"
      },
      "source": [
        "##instantiate tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "##create corpus by lowering the letters and splitting the text by \\n\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6g7n_VNfuXh"
      },
      "source": [
        "### Step 2: Train the tokenizer and create word encoding dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEpXxNmgfwJW"
      },
      "source": [
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "##calculate vocabulary size - +1 for <oov> token\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgja4DLxoE3T"
      },
      "source": [
        "### Step 3: Create N-gram sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9stgiKFjoGWS"
      },
      "source": [
        "##create n-gram sequences of each text sequence\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    tokens = tokenizer.texts_to_sequences([line])[0]  # get all the tokens of the sequence\n",
        "    for i in range(1, len(tokens)):  # create n-gram sequences\n",
        "        n_gram_sequence = tokens[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNuPpCQ82Nx5"
      },
      "source": [
        "input_sequences[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsdMaP2WoPJ8"
      },
      "source": [
        "##pad sequences\n",
        "max_seq_len = max([len(i) for i in input_sequences])\n",
        "input_seq_array = np.array(pad_sequences(input_sequences,\n",
        "                                         maxlen=max_seq_len,\n",
        "                                         padding='pre')\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2giUjTv12F82"
      },
      "source": [
        "input_seq_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMT7Xdc1oVWs"
      },
      "source": [
        "### Step 4: Extract features and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXU2dLrqoaf4"
      },
      "source": [
        "##creating features(X) and label(y)\n",
        "X = input_seq_array[:, :-1]\n",
        "labels = input_seq_array[:, -1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWEeIpo14S0W"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcTmOXXm4QNL"
      },
      "source": [
        "##one-hot encode the labels to get y\n",
        "y = tf.keras.utils.to_categorical(labels, num_classes=vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lViBfsjW4agz"
      },
      "source": [
        "print(X[2])\n",
        "print(y[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrgODC676Ln5"
      },
      "source": [
        "### Define the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zULu3Flv6SHS"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "                tf.keras.layers.Embedding(vocab_size, 64, input_length=max_seq_len-1),\n",
        "                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "                tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IXLQ2K56XC3"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK5xoL9p6Zef"
      },
      "source": [
        "history = model.fit(X, y, epochs=500, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMO9pyt58Qk4"
      },
      "source": [
        "### Visualize metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PRAvT028USk"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_metric(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRxOpqCH8aVW"
      },
      "source": [
        "plot_metric(history, 'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxxSWpCF9L14"
      },
      "source": [
        "### Generate new text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7nBMahi9NcN"
      },
      "source": [
        "seed_text = \"It was a cold night.\"\n",
        "\n",
        "##add number of words you want to predict\n",
        "next_words = 100\n",
        "  \n",
        "##run the loop to predict and concatenate the word\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
        " \n",
        "    ##predict the class using the trained model\n",
        "    predict1 = model.predict(token_list, verbose=0)\n",
        "    predicted = np.argmax(predict1,axis=1)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        ##reference the predicted class with the vocabulary\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BO0hLHSDdP0"
      },
      "source": [
        "### Activity: Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPDi_K_HKvMh"
      },
      "source": [
        "##import the required libraries and APIs\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpFP1jXyKxq1"
      },
      "source": [
        "### Step 1: Create a corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S49L25AtKzG0"
      },
      "source": [
        "##download data from this url\n",
        "!wget --no-check-certificate \\\n",
        "    https://raw.githubusercontent.com/dswh/lil_nlp_with_tensorflow/main/sonnets.txt \\\n",
        "    -O /tmp/sonnet.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wiotmguK104"
      },
      "source": [
        "##printing the text\n",
        "shakespeare_text = open('/tmp/sonnet.txt').read()\n",
        "print(len(shakespeare_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xPdPXoUK9y9"
      },
      "source": [
        "##create corpus by lowering the letters and splitting the text by \\n\n",
        "corpus = shakespeare_text.____().____(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK00cYcTLAYV"
      },
      "source": [
        "### Set up the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gfchet1PLEGm"
      },
      "source": [
        "##set up tokenizer\n",
        "tokenizer = Tokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xpafpg3VLG1q"
      },
      "source": [
        "tokenizer.fit_on_texts(____)\n",
        "\n",
        "##calculate vocabulary size - be mindful of the <oov> token\n",
        "vocab_size = len(tokenizer.____) + __\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHmvDhozLK61"
      },
      "source": [
        "##create sequences of \n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    tokens = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(tokens)):\n",
        "        n_gram_sequence = tokens[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tljlRZhLNHu"
      },
      "source": [
        "##pad sequences\n",
        "max_seq_len = max([len(i) for i in ____])\n",
        "input_seq_array = np.array(pad_sequences(input_sequences,\n",
        "                                         maxlen=____,\n",
        "                                         padding='____')\n",
        "                        )\n",
        "​"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAlFCXkVLZBz"
      },
      "source": [
        "##creating features(X) and label(y)\n",
        "X = input_seq_array[:, ____]\n",
        "labels = input_seq_array[:, __]\n",
        "\n",
        "##one-hot encode the labels to get y - since it is actually just a classification problem\n",
        "y = tf.keras.utils.____(____, num_classes=____)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y_brbmlLb0K"
      },
      "source": [
        "### Define the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2Ftso3jLeVq"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "                tf.keras.layers.____(vocab_size, 120, input_length=max_seq_len-1),\n",
        "                tf.keras.layers.____(tf.keras.layers.____(120)),\n",
        "                tf.keras.layers.____(vocab_size, activation='____')\n",
        "])\n",
        "\n",
        "##define the learning rate - step size for optimizer\n",
        "adam = tf.keras.optimizers.Adam(lr=0.01)\n",
        "\n",
        "model.compile(loss='____', optimizer=adam, metrics=['accuracy'])\n",
        "history = model.fit(X, y, epochs=200, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwQDi0y2LuAF"
      },
      "source": [
        "### Visualise the metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaCXkmkqLvud"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_metric(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2Txupf2L3Ay"
      },
      "source": [
        "plot_metric(history, 'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgma4YMeKlL2"
      },
      "source": [
        "### Solution: Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqX9eN8cDfr6"
      },
      "source": [
        "##import the required libraries and APIs\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4rsd5XjDqdS"
      },
      "source": [
        "### Step 1: Create a corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUS_eseUDvKL"
      },
      "source": [
        "##download data from this url\n",
        "!wget --no-check-certificate \\\n",
        "    https://raw.githubusercontent.com/dswh/lil_nlp_with_tensorflow/main/sonnets.txt \\\n",
        "    -O /tmp/sonnet.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wzl_oOWKFD5e"
      },
      "source": [
        "##printing the text\n",
        "shakespeare_text = open('/tmp/sonnet.txt').read()\n",
        "print(len(shakespeare_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nohUQJwYFnvH"
      },
      "source": [
        "##create corpus by lowering the letters and splitting the text by \\n\n",
        "corpus = shakespeare_text.lower().split(\"\\n\")\n",
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsRp3Dt2GMBL"
      },
      "source": [
        "### Step 2: Train the tokenizer and create word encoding dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk3fmRbkGVYf"
      },
      "source": [
        "##set up tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "##calculate vocabulary size - be mindful of the <oov> token\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KnzcKZuGeKF"
      },
      "source": [
        "### Step 3: Create N-gram sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKBJkipzHcOM"
      },
      "source": [
        "##create sequences of \n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    tokens = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(tokens)):\n",
        "        n_gram_sequence = tokens[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYUIKSZzHfLW"
      },
      "source": [
        "##pad sequences\n",
        "max_seq_len = max([len(i) for i in input_sequences])\n",
        "input_seq_array = np.array(pad_sequences(input_sequences,\n",
        "                                         maxlen=max_seq_len,\n",
        "                                         padding='pre')\n",
        "                        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4IHHIKUHiBj"
      },
      "source": [
        "### Step 4: Extract features and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QURTpRbHpqz"
      },
      "source": [
        "##creating features(X) and label(y)\n",
        "X = input_seq_array[:, :-1]\n",
        "labels = input_seq_array[:, -1]\n",
        "\n",
        "##one-hot encode the labels to get y - since it is actually just a classification problem\n",
        "y = tf.keras.utils.to_categorical(labels, num_classes=vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWKqCcY_Ie8e"
      },
      "source": [
        "### Define the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7Zff0s-IhxG"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "                tf.keras.layers.Embedding(vocab_size, 120, input_length=max_seq_len-1),\n",
        "                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(120)),\n",
        "                tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "##define the learning rate - step size for optimizer\n",
        "adam = tf.keras.optimizers.Adam(lr=0.01)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "history = model.fit(X, y, epochs=200, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4orfvV5fJE1n"
      },
      "source": [
        "### Visualise the metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMmEesXkJIYY"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_metric(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt1qdB_UJLIh"
      },
      "source": [
        "plot_metric(history, 'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc-Jr2dKJhAK"
      },
      "source": [
        "### Generate new text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC7Mg3nfJlJm"
      },
      "source": [
        "seed_text = \"It was a cold night.\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
        "    predict1 = model.predict(token_list, verbose=0)\n",
        "    predicted = np.argmax(predict1,axis=1)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPOxzcAxdjWZ"
      },
      "source": [
        "# Topic 3 Introduction to Attention Mechanism and Transformer Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT8dx6KbkjPb"
      },
      "source": [
        "## The Transformer encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0lHX7IAkotr"
      },
      "source": [
        "### Getting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrtnez538TdU"
      },
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edsQEp2dksox"
      },
      "source": [
        "### Preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K47LneFi6UuE"
      },
      "source": [
        "import os, pathlib, shutil, random\n",
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DVVQWBuj6Rt"
      },
      "source": [
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DGI06Bgkzyo"
      },
      "source": [
        "### Vectorizing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80InZpnNdjrX"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4A3XP52Tk5JD"
      },
      "source": [
        "### Transformer encoder implemented as a subclassed `Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozZ-RLUr8dx0"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri83bnlik8n0"
      },
      "source": [
        "### Using the Transformer encoder for text classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xal6kWmekNMq"
      },
      "source": [
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrrs7BYhlAm4"
      },
      "source": [
        "### Training and evaluating the Transformer encoder based model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cte8kQNlkP_t",
        "outputId": "9a332c3d-38f7-4744-c64e-6faa4c509f2b"
      },
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r248/782 [========>.....................] - ETA: 29:07 - loss: 0.6408 - accuracy: 0.6895"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv_jV9XolE_B"
      },
      "source": [
        "### Implementing positional embedding as a subclassed layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkNxcjqlkZmr"
      },
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hn9ZBQablJQm"
      },
      "source": [
        "### Combining the Transformer encoder with positional embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SssVb9AqkcU4"
      },
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"full_transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS5K6YyEQGBa"
      },
      "source": [
        "# Appendix 1: Text Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qfe-LwPNnL7"
      },
      "source": [
        "## Bag-of-words approach: Unigram, Bigram, TD-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c8nhZSJNq2M"
      },
      "source": [
        "### Preparing the IMDB movie reviews data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjdJdRhc8kbp"
      },
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9r7i5eT8oPJ"
      },
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgAHuw4LMZtF"
      },
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOOsAl79MbTg"
      },
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9vAYb76MdSj"
      },
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFE7i17IN04M"
      },
      "source": [
        "#### Displaying the shapes and dtypes of the first batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VmTuNOWMfRK"
      },
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PHx_ZFYNZSP"
      },
      "source": [
        "### Unigram with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y28FUwk4NWTd"
      },
      "source": [
        "#### Preprocessing our datasets with a TextVectorization layer\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ohBEJyiMkMP"
      },
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dlk0M6_WNDr6"
      },
      "source": [
        "#### Inspecting the output of our binary unigram dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ltvmypROOkP"
      },
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXemtsRNM_TE"
      },
      "source": [
        "#### Our model-building utility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4fBy_OXMnkQ"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-wKXfCzM7QY"
      },
      "source": [
        "#### Training and testing the binary unigram model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9vevtGoMrUH"
      },
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3Q4ucmkO4TY"
      },
      "source": [
        "### Bigrams with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnaFlTteO8bG"
      },
      "source": [
        "#### Configuring the `TextVectorization` layer to return bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9EESyjfM1Xp"
      },
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pomR-4UXPD-G"
      },
      "source": [
        "#### Training and testing the binary bigram model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4nH6wVkPH7Q"
      },
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjNu3zqhPrv2"
      },
      "source": [
        "### Bigrams with TF-IDF encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlU5Q6L0PxGR"
      },
      "source": [
        "#### Configuring TextVectorization to return TF-IDF-weighted outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-vZK-JhPLc_"
      },
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD1nULeFP486"
      },
      "source": [
        "#### Training and testing the TF-IDF bigram model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhVXhmZkP2HR"
      },
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWtk1VxPWZBe"
      },
      "source": [
        "# Appendix 2: Glove Pre-trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO5T4u8sYVur"
      },
      "source": [
        "## The sequence model approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnuyrVt5Ycjx"
      },
      "source": [
        "### Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfoZMVNmP8RS"
      },
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXT7gFgVYjed"
      },
      "source": [
        "### Preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_WnP9nMYfsj"
      },
      "source": [
        "import os, pathlib, shutil, random\n",
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkkMv59wY6-_"
      },
      "source": [
        "### Preparing integer sequence datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekP78WJAYnYl"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfYvnwT1ZAV9"
      },
      "source": [
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiaEfgC4Z-Vd"
      },
      "source": [
        "### A sequence model built on one-hot encoded vector sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3LNsmrEZjJ-"
      },
      "source": [
        "import tensorflow as tf\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swqRdNt-aD2D"
      },
      "source": [
        "### Training a first basic sequence model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtXSZT4WaBjg"
      },
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvjJBgDcaG_q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mhgc8y-saPAz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}